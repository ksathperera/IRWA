{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNYt3+gjollBPg4Sj+kRm1N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"luLjo3Qu_rjG","executionInfo":{"status":"ok","timestamp":1695794280812,"user_tz":-330,"elapsed":28398,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"5a1e210c-dd34-4946-c9b9-22d18ea254cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#to load the file path\n","%cd /content/drive/MyDrive/Colab Notebooks/IRWA/Lab 02/Lab 02 Materials\n","#/content/drive/MyDrive/Colab Notebooks/IRWA/Lab 02/Lab 02 Materials/inverted\n","#dictionary=os.getcws()\n","#print(dictionary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t_8Fc-qzA7Su","executionInfo":{"status":"ok","timestamp":1695794284364,"user_tz":-330,"elapsed":618,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"bb7c6de5-0ff1-488c-c08b-f4eda3867e49"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/IRWA/Lab 02/Lab 02 Materials\n"]}]},{"cell_type":"code","source":["#to get current working directory\n","import os\n","directory = os.getcwd()+'/inverted'\n","print(directory)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6HWuB7WB67K","executionInfo":{"status":"ok","timestamp":1695794286100,"user_tz":-330,"elapsed":5,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"32cf29f2-7a43-4c0b-bf9b-69d7a750aba4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/IRWA/Lab 02/Lab 02 Materials/inverted\n"]}]},{"cell_type":"code","source":["#to see the file that are in that inverted folder\n","files = os.listdir(directory)\n","print(files)"],"metadata":{"id":"Lk3V-BHYEUPE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695794288768,"user_tz":-330,"elapsed":616,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"09037a89-eff0-4aec-e020-3fc394b5e767"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['Doc1.txt', 'Doc2.txt', 'Doc3.txt', 'Doc4.txt']\n"]}]},{"cell_type":"code","source":["#to read d1 and split words\n","with open(\"/content/drive/MyDrive/Colab Notebooks/IRWA/Lab 02/Lab 02 Materials/inverted/Doc1.txt\",'r') as d1:\n","    print(d1.read().split())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3hHo5d4Pz-EK","executionInfo":{"status":"ok","timestamp":1695794326353,"user_tz":-330,"elapsed":621,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"e2efcd5d-d9b5-4ae2-ad98-e72251a1b66d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['breakthrough', 'drug', 'for', 'schizophrenia']\n"]}]},{"cell_type":"markdown","source":["Q1 a)\n","\n"," Write the code to build the inverted index for the above corpus"],"metadata":{"id":"pmOteKP6Rt-p"}},{"cell_type":"code","source":["#wordDict variable is initialized as an empty dictionary.\n","#This dictionary will store the inverted index,\n","#where each term will be a key and the corresponding value will be a list of documents in which the term appears.\n","wordDict=dict()\n"],"metadata":{"id":"Im1YBgBQ3sKJ","executionInfo":{"status":"ok","timestamp":1695795193273,"user_tz":-330,"elapsed":4,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Building the inverted index: The code iterates through each file in the files.\n","\n","a. For each file, it opens the file using the open() function, reading the contents of the file.\n","\n","b. The contents of the file are converted to lowercase using the .lower() method, and then split into individual words using the .split() method. This creates a list of words present in the file.\n","\n","c. The code then iterates through each word in the list of words.\n","\n","d. For each word, it checks if it is already present in the wordDict dictionary.\n","\n","If the word is not present in wordDict, it adds the word as a key and initializes the corresponding value as a list containing the current file. If the word is already present in wordDict, it appends the current file to the existing list of files associated with that word."],"metadata":{"id":"wIUPmLTDQBm9"}},{"cell_type":"code","source":["#to open and read files one by one\n","for file in files:\n","  with open(os.getcwd()+'/inverted/'+file,'r') as f:\n","    #print(f)\n","    words=f.read().lower().split()\n","    #print(words)\n","\n","    #to read\n","    for word in words:\n","      if word not in wordDict:\n","        wordDict[word] = [file]\n","      else:\n","        wordDict[word]+=[file]\n","        #wordDict[word].append(file)\n","\n","print(wordDict)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695803208169,"user_tz":-330,"elapsed":520,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"ef49b164-91d4-4d6e-cdae-e0cc21632dc8","id":"O8kz3m4QRjeM"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["{'breakthrough': ['Doc1.txt', 'Doc1.txt'], 'drug': ['Doc1.txt', 'Doc2.txt', 'Doc1.txt', 'Doc2.txt'], 'for': ['Doc1.txt', 'Doc3.txt', 'Doc4.txt', 'Doc1.txt', 'Doc3.txt', 'Doc4.txt'], 'schizophrenia': ['Doc1.txt', 'Doc2.txt', 'Doc3.txt', 'Doc4.txt', 'Doc1.txt', 'Doc2.txt', 'Doc3.txt', 'Doc4.txt'], 'new': ['Doc2.txt', 'Doc3.txt', 'Doc4.txt', 'Doc2.txt', 'Doc3.txt', 'Doc4.txt'], 'approach': ['Doc3.txt', 'Doc3.txt'], 'treatment': ['Doc3.txt', 'Doc3.txt'], 'of': ['Doc3.txt', 'Doc3.txt'], 'hopes': ['Doc4.txt', 'Doc4.txt'], 'patients': ['Doc4.txt', 'Doc4.txt']}\n"]}]},{"cell_type":"markdown","source":["# Q1 B)\n"," Write suitable code to do the following retrieval tasks.\n","\n","I. schizophrenia AND drug\n","\n","II. for AND NOT(drug OR approach)"],"metadata":{"id":"O2UU01mASfJ5"}},{"cell_type":"markdown","source":["For this we haev to reaturn the dictioanry from previous one, and make a function for this"],"metadata":{"id":"UaDoDlA5Tbc7"}},{"cell_type":"code","source":["#I. schizophrenia AND drug\n","\n","\n","def InvertedIndex():\n","  wordDict = dict()\n","  for file in files:\n","    with open(os.getcwd()+'/inverted/'+file,'r') as f:\n","      #print(f)\n","      words=f.read().lower().split()\n","      #print(words)\n","\n","      #to read\n","      for word in words:\n","        if word not in wordDict:\n","          wordDict[word] = [file]\n","        else:\n","          wordDict[word].append(file)\n","\n","  return wordDict\n","\n"],"metadata":{"id":"474xh3P9Smul","executionInfo":{"status":"ok","timestamp":1695803243291,"user_tz":-330,"elapsed":824,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["#to get the AND operators in that two terms\n","\n","def And_op(list1,list2):\n","  if (list1 and list2):\n","    return set(list1).intersection(list2)\n","  else:\n","    return()\n","\n","#what is this list1 and list 2, they are lists that shows which document containing that two terms\n","\n","ii=InvertedIndex()\n","for key in ii:\n","  if key == 'schizophrenia':\n","    list1=ii[key]\n","    print('list1 : ', list1)\n","  if key == 'drug':\n","    list2 = ii[key]\n","    print('list2 :', list2)\n","\n","print()\n","print()\n","\n","#to intersect that two terms using and opeartor\n","print('schizophrenia AND drug :', And_op(list1,list2))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DD-PWsTPUgaz","executionInfo":{"status":"ok","timestamp":1695803247362,"user_tz":-330,"elapsed":801,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"3e939580-3654-4925-dfcb-fb6661b15369"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["list2 : ['Doc1.txt', 'Doc2.txt']\n","list1 :  ['Doc1.txt', 'Doc2.txt', 'Doc3.txt', 'Doc4.txt']\n","\n","\n","schizophrenia AND drug : {'Doc2.txt', 'Doc1.txt'}\n"]}]},{"cell_type":"code","source":["#II. for AND NOT(drug OR approach)\n","\n","def OR_op(list1,list2):\n","  if(list1 or list2):\n","    return set(list1).union(list2)\n","  else:\n","    return()\n","\n","def NOT_op(a):\n","  directory=os.getcwd()+'/inverted'\n","  filelists = os.listdir(directory)\n","  return set(filelists).symmetric_difference(a)\n","\n","\n","for key in ii:\n","  if key == 'drug':\n","    list1=ii[key]\n","    print('drug ',list1)\n","  if key == 'approach':\n","    list2 = ii[key]\n","    print('approach ',list2)\n","  if key == 'for':\n","    list3 = ii[key]\n","    print('for ',list3)\n","\n","print()\n","\n","list4 = OR_op(list1,list2)\n","\n","list5 = NOT_op(list4)\n","\n","print('for AND NOT(drug OR approach) ', And_op(list3,list5) )\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJUQFnzCVi9G","executionInfo":{"status":"ok","timestamp":1695804679132,"user_tz":-330,"elapsed":615,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"fb071e72-895b-4968-cdad-25dc083a2edb"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["drug  ['Doc1.txt', 'Doc2.txt']\n","for  ['Doc1.txt', 'Doc3.txt', 'Doc4.txt']\n","approach  ['Doc3.txt']\n","\n","for AND NOT(drug OR approach)  {'Doc4.txt'}\n"]}]},{"cell_type":"code","source":["InvertedIndex()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AF_BkN2qyAXl","executionInfo":{"status":"ok","timestamp":1695803773509,"user_tz":-330,"elapsed":5,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"84dbb1dd-00ce-484d-c8f4-ddba8b94bdfb"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'breakthrough': ['Doc1.txt'],\n"," 'drug': ['Doc1.txt', 'Doc2.txt'],\n"," 'for': ['Doc1.txt', 'Doc3.txt', 'Doc4.txt'],\n"," 'schizophrenia': ['Doc1.txt', 'Doc2.txt', 'Doc3.txt', 'Doc4.txt'],\n"," 'new': ['Doc2.txt', 'Doc3.txt', 'Doc4.txt'],\n"," 'approach': ['Doc3.txt'],\n"," 'treatment': ['Doc3.txt'],\n"," 'of': ['Doc3.txt'],\n"," 'hopes': ['Doc4.txt'],\n"," 'patients': ['Doc4.txt']}"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["Extra *Notes*\n"],"metadata":{"id":"hmGj76B407cu"}},{"cell_type":"code","source":["#note\n","L=[10,20,30,40,100,200,400]\n","\n","L+[1,2,3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j0PfD9keVu70","executionInfo":{"status":"ok","timestamp":1695803516399,"user_tz":-330,"elapsed":1551,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"35cdb690-be05-4177-98ad-1f3742d32a05"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[10, 20, 30, 40, 100, 200, 400, 1, 2, 3]"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["set[L]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qyr0OlG7xBjr","executionInfo":{"status":"ok","timestamp":1695803529395,"user_tz":-330,"elapsed":516,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"2a76ae34-d0ac-4978-8450-8bd731c1d01a"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["set[[10, 20, 30, 40, 100, 200, 400]]"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["a={1,2,3,4,5}\n","b={3,4,5,6,8}\n","\n","a.intersection(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_PXXAw0xFOr","executionInfo":{"status":"ok","timestamp":1695803556592,"user_tz":-330,"elapsed":579,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"ede8d72d-567c-414c-e01e-bf3f97512c6e"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{3, 4, 5}"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["NLTK\n","\n","NLTK is a platform which support to build Python programs to work with human language data.\n","It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,\n","along with a suite of text processing libraries for classification, tokenization, stemming, tagging,\n","parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n","\n","Installing NLTK\n","\n","NLTK requires Python versions 3.5, 3.6, 3.7, 3.8, or 3.9\n","\n","Windows\n","\n","• Install Numpy (optional): https://www.scipy.org/scipylib/download.html\n","pip install numpy\n","\n","• Install NLTK: http://pypi.python.org/pypi/nltk\n","Pip install nlt"],"metadata":{"id":"2_2sH_yU2qoY"}},{"cell_type":"code","source":["#installing nltk and required libraries\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize  #to split words"],"metadata":{"id":"e62xxgmCxdAZ","executionInfo":{"status":"ok","timestamp":1695808820705,"user_tz":-330,"elapsed":458,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["#you have to download stopwords from nltk library separately\n","import nltk\n","nltk.download('stopwords')  #for stopwords\n","nltk.download('punkt')   #for tokenizing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sHLmVO9GZUy","executionInfo":{"status":"ok","timestamp":1695809119379,"user_tz":-330,"elapsed":1352,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"347f4494-bf06-43ae-8cc3-f5358bab9c00"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["#to print the list of stopwords in that library\n","stopwords = stopwords.words('english')\n","print(stopwords)\n","#to see the length of that stopwords library\n","print(len(stopwords))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5byxlXKXFjdf","executionInfo":{"status":"ok","timestamp":1695808898317,"user_tz":-330,"elapsed":457,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"78e0d7e6-ff73-4496-9775-e1ae9d1aa691"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","179\n"]}]},{"cell_type":"markdown","source":["**Importing NLTK libraries:** The code imports necessary libraries from the Natural Language Toolkit (NLTK), including stopwords and word_tokenize. These libraries provide functionalities for dealing with stopwords (common words to be ignored) and tokenizing text into individual words."],"metadata":{"id":"3tzNyh7jEHwo"}},{"cell_type":"markdown","source":["It's not enough to import from nltk.corpus import stopwords\n","\n","Have to download nltk.download('stopwords') too\n","\n"],"metadata":{"id":"Vgnb3ObnC8E0"}},{"cell_type":"markdown","source":["Q2\n","\n","a) Remove stop words in the given string using nltk library\n","\n","quote = \"Pythoners are very intelligent and work very pythonly and n\n","ow they are pythoning their way to success.\""],"metadata":{"id":"O2HHW5dvEiz9"}},{"cell_type":"code","source":["quote = \"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n","\n","#first have to import the from nltk.tokenize import word_tokenize\n","#and after that you have to dowload('punkt') as well\n","\n","tokenized_words = word_tokenize(quote)\n","print(tokenized_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-j3gca_MGDl2","executionInfo":{"status":"ok","timestamp":1695809618159,"user_tz":-330,"elapsed":449,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"03fcfbed-2cde-45f1-b4a9-1bac072bb783"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n"]}]},{"cell_type":"code","source":["#now remove stopwords\n","new_list_without_stopwords = []\n","for tokenized_w in tokenized_words:  #we have to go through each and every tokenized words\n","  if tokenized_w not in stopwords:    #check stopwords is metched to that tokenized words\n","    new_list_without_stopwords.append(tokenized_w)\n","print(new_list_without_stopwords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wsf7xSPGG_gY","executionInfo":{"status":"ok","timestamp":1695809796693,"user_tz":-330,"elapsed":465,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"caa724e8-4546-49d3-a3b2-0b041759d788"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["['Pythoners', 'intelligent', 'work', 'pythonly', 'pythoning', 'way', 'success', '.']\n"]}]},{"cell_type":"code","source":["#method 1\n","#to add new stopwords to the stopwords list\n","stopwords.append('intelligent')\n","stopwords.append('work')\n","\n","print(len(stopwords))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5gM2YEveIOv3","executionInfo":{"status":"ok","timestamp":1695809957878,"user_tz":-330,"elapsed":799,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"dc9801a0-6a6b-4b57-bc7c-561520ae0bf2"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["181\n"]}]},{"cell_type":"code","source":["#to see without work and intelligent words\n","new_list_without_addedstopwords = []\n","for tokenized_w in tokenized_words:\n","  if tokenized_w not in stopwords:\n","    new_list_without_addedstopwords.append(tokenized_w)\n","\n","print(new_list_without_addedstopwords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4muF3_CJlkQ","executionInfo":{"status":"ok","timestamp":1695810914892,"user_tz":-330,"elapsed":451,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"f33a05d1-26f9-49e3-b986-73af19bbb5f6"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["['Pythoners', 'pythonly', 'pythoning', 'way', 'success', '.']\n"]}]},{"cell_type":"code","source":["#method 2\n","import nltk\n","from nltk.corpus import stopwords\n","\n","new_stopwords_list = ['intelligent','work']\n","nltk_stopwords = stopwords.words('english')\n","\n","final_stopwords_list = new_stopwords_list + stopwords.words('english')\n","\n","print(final_stopwords_list)\n","print(len(final_stopwords_list))\n","print()\n","\n","#to see without work and intelligent words\n","new_list_without_addedstopwords = []\n","for tokenized_w in tokenized_words:\n","  if tokenized_w not in final_stopwords_list:\n","    new_list_without_addedstopwords.append(tokenized_w)\n","\n","print(new_list_without_addedstopwords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5HTBnSP3PPFv","executionInfo":{"status":"ok","timestamp":1695811592488,"user_tz":-330,"elapsed":490,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"158449bb-6dca-4674-e3fa-8695ff7a9bda"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["['intelligent', 'work', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","181\n","\n","['Pythoners', 'pythonly', 'pythoning', 'way', 'success', '.']\n"]}]},{"cell_type":"markdown","source":["Q2\n","\n","b)\n","**Stemming**\n","\n","For stemming we can use potter stemmer algorithm\n","\n","In stemming meaning of the words are differ sometimes\n","\n","It's just cutting letters without any propper manner"],"metadata":{"id":"xcIF0_mWQL4u"}},{"cell_type":"code","source":["import nltk\n","from nltk.stem import PorterStemmer"],"metadata":{"id":"rjLG05MMPsRQ","executionInfo":{"status":"ok","timestamp":1695812032630,"user_tz":-330,"elapsed":2,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["quote = \"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n"],"metadata":{"id":"sbgodA5SRLNV","executionInfo":{"status":"ok","timestamp":1695812030323,"user_tz":-330,"elapsed":452,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["stemmed_words=[]\n","stemmer = PorterStemmer()\n","for word in tokenized_words:\n","  stemmed_words.append(stemmer.stem(word))\n","\n","print(stemmed_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXKAFWMKSDXd","executionInfo":{"status":"ok","timestamp":1695812411964,"user_tz":-330,"elapsed":630,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"d522d9bc-2866-4988-e5d5-bffed3986568"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["['python', 'are', 'veri', 'intellig', 'and', 'work', 'veri', 'pythonli', 'and', 'now', 'they', 'are', 'python', 'their', 'way', 'to', 'success', '.']\n"]}]},{"cell_type":"code","source":["#to stem before tokenized words\n","stemmed_words=[]\n","stemmer = PorterStemmer()\n","for word in new_list_without_stopwords:\n","  stemmed_words.append(stemmer.stem(word))\n","\n","print(stemmed_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8u9LIdI8SLQO","executionInfo":{"status":"ok","timestamp":1695812453859,"user_tz":-330,"elapsed":663,"user":{"displayName":"it21178368 Perera K.K.S.","userId":"16594895165566436229"}},"outputId":"73adcb34-78c3-471f-9d77-c002f11a1166"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["['python', 'intellig', 'work', 'pythonli', 'python', 'way', 'success', '.']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"TSX9WKzOTIB1"},"execution_count":null,"outputs":[]}]}